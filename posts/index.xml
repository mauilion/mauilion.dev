<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Duffie Cooley</title>
		<link>https://mauilion.dev/posts/</link>
		<description>Recent content in Posts on Duffie Cooley</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Wed, 17 Apr 2019 10:44:33 -0700</lastBuildDate>
		<atom:link href="https://mauilion.dev/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Using MetalLb with Kind</title>
			<link>https://mauilion.dev/posts/kind-metallb/</link>
			<pubDate>Wed, 17 Apr 2019 10:44:33 -0700</pubDate>
			
			<guid>https://mauilion.dev/posts/kind-metallb/</guid>
			<description>Preamble: When using metallb with kind we are going to deploy it in l2-mode. This means that we need to be able to connect to the ip addresses of the node subnet. If you are using kind on Mac or Windows this will require you to add a route to your laptop routing the subnet used by your kind cluster to the vm that is hosting it. If you are using linux to host a kind cluster.</description>
			<content type="html"><![CDATA[

<h3 id="preamble">Preamble:</h3>

<p>When using metallb with kind we are going to deploy it in <code>l2-mode</code>. This means that we need to be able to connect to the ip addresses of the node subnet.
If you are using kind on Mac or Windows this will require you to add a route to your laptop routing the subnet used by your kind cluster to the vm that is hosting it.
If you are using linux to host a kind cluster. You will not need to do this as the kind node ip addresses are directly attached.</p>

<h3 id="problem-statement">Problem Statement:</h3>

<p>Kubernetes on bare metal doesn&rsquo;t come with an easy integration for things like services of type LoadBalancer.</p>

<p>This mechanism is used to expose services inside the cluster using an external Load Balancing mechansim that understands how to route traffic down to the pods defined by that service.</p>

<p>Most implementations of this are relatively naive. They place all of the available nodes behind the load balancer and use tcp port knocking to determine if the node is &ldquo;healthy&rdquo; enough to forward traffic to it.</p>

<p>With Metallb there are a different set of assumptions.</p>

<p>Metallb can operate in two distinct modes.</p>

<p>A Layer 2 mode that will use vrrp to arp out for the external ip or VIP on the lan.</p>

<p>A bgp mode that will announce the external ip or VIP from all of the nodes where at least one pod is running.</p>

<p>the bgp mode relies on ecmp to balance traffic back to the pods. ECMP is a great solution for this problem and I HIGHLY recommend you use this model if you can.</p>

<p>That said I haven&rsquo;t created a bgp router for my kind cluster so we wil use the l2-mode for this experiment.</p>

<h3 id="let-s-do-this-thing">Let&rsquo;s do this thing!</h3>

<p>First let&rsquo;s bring up a 2 node kind cluster with the following config.</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">kind<span class="p">:</span><span class="w"> </span>Cluster<span class="w">
</span><span class="w"></span>apiVersion<span class="p">:</span><span class="w"> </span>kind.sigs.k8s.io/v1alpha3<span class="w">
</span><span class="w"></span>nodes<span class="p">:</span><span class="w">
</span><span class="w"></span>-<span class="w"> </span>role<span class="p">:</span><span class="w"> </span>control-plane<span class="w">
</span><span class="w"></span>-<span class="w"> </span>role<span class="p">:</span><span class="w"> </span>worker</code></pre></div>
<p>
    <asciinema-player
        src="/casts/km-bringup.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:1:00"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>Then we need to see if we can ping the node ip of the nodes themselves.</p>

<p>
    <asciinema-player
        src="/casts/km-ping.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:1:00"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>At this point we need to determine the network that is being used for the node ip pool. Since kind nodes are associated with the docker network named &ldquo;bridge&rdquo; we can inspect that directly.</p>

<p>I am using a pretty neat tool called <a href="https://github.com/simeji/jid" rel="nofollow noreferrer" target="_blank"><code>jid</code></a> here that is a repl for json.</p>

<p>
    <asciinema-player
        src="/casts/km-inspect.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:0:16"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>So we can see that there is an allocated network of <code>172.17.0.0/16</code> in my case.</p>

<p>Let&rsquo;s swipe the last 10 ip addresses from that allocation and use them for the metallb configuration.</p>

<h3 id="now-we-are-going-to-deploy-a-service">Now we are going to deploy a service!</h3>

<p>First let&rsquo;s create a service of type loadbalancer and see what happens before we install metallb.</p>

<p>I am going to use the echo server for this. I prefer the one built by inanimate. Here is the <a href="https://github.com/InAnimaTe/echo-server" rel="nofollow noreferrer" target="_blank">source</a> and image: <code>inanimate/echo-server</code></p>

<p>
    <asciinema-player
        src="/casts/km-echo1.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:1:00"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>We can see that the <code>EXTERNAL-IP</code> field is <code>pending</code>. This is because there is nothing available in the cluster to manage this type of service.</p>

<h3 id="now-on-to-the-metallb-part">Now on to the metallb part!</h3>

<p>First read the docs <a href="https://metallb.universe.tf/installation/" rel="nofollow noreferrer" target="_blank">https://metallb.universe.tf/installation/</a></p>

<p>Then we can get started on installing this to our cluster.</p>

<p>
    <asciinema-player
        src="/casts/km-metallb-install.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:1:00"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>We can see that metallb is now installed but we aren&rsquo;t done yet!</p>

<p>now we need to add a configuration that will use a few of the unused ip addresses from the node ip pool (<code>172.17.0.0/16</code>)</p>

<p>Now if we look at our existing service we can see that the <code>EXTERNAL-IP</code> is still <code>pending</code></p>

<p>This is because we haven&rsquo;t yet applied the config for metallb.</p>

<p>Here is the config:</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>ConfigMap<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>namespace<span class="p">:</span><span class="w"> </span>metallb-system<span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>config<span class="w">
</span><span class="w"></span>data<span class="p">:</span><span class="w">
</span><span class="w">  </span>config<span class="p">:</span><span class="w"> </span><span class="sd">|
</span><span class="sd">    address-pools:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>default<span class="w">
</span><span class="w">      </span>protocol<span class="p">:</span><span class="w"> </span>layer2<span class="w">
</span><span class="w">      </span>addresses<span class="p">:</span><span class="w">
</span><span class="w">      </span>-<span class="w"> </span><span class="m">172.17</span>.<span class="m">255.1</span>-<span class="m">172.17</span>.<span class="m">255.250</span></code></pre></div>
<p>You can apply this to your cluster with <code>kubectl apply -f https://git.io/km-config.yaml</code></p>

<p>Let&rsquo;s see what happens when we apply this.</p>

<p>
    <asciinema-player
        src="/casts/km-config.cast"
        cols="200"
        rows="28"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:0:02"
        
        
        
        
        
        
    ></asciinema-player>
</p>

<p>We can see the svc get&rsquo;s an ip address immediatly.</p>

<p>And we can curl it!</p>

<p>That&rsquo;s all for now hit me up on <a href="https://twitter.com/mauilion" rel="nofollow noreferrer" target="_blank">twitter</a> or <a href="https://kubernetes.slack.com/team/U37TLLWAU" rel="nofollow noreferrer" target="_blank">k8s slack</a> with questions!</p>

<p>Shout-out to Jan Guth for the idea on this post!</p>
]]></content>
		</item>
		
		<item>
			<title>Presenting to the San Francisco Kubernetes Meetup about kind!&#34;</title>
			<link>https://mauilion.dev/posts/kind-demo/</link>
			<pubDate>Wed, 10 Apr 2019 15:28:10 -0700</pubDate>
			
			<guid>https://mauilion.dev/posts/kind-demo/</guid>
			<description>On 4/7/2019 I had the opportunity to talk to folks that attended the SF Kubernetes meetup Anaplan about kind!
It&amp;rsquo;s a great project and I end up using kind everyday to validate or develop designs for Kubernetes clusters.
The slides that I presented are here: mauilion.github.io/kind-demo and a link to the repository with the deck and the content used to bring up the demo cluster is here: github.com/mauilion/kind-demo

In the talk I dug in a bit about what kind and kubeadm are and what problems they solve.</description>
			<content type="html"><![CDATA[<p>On 4/7/2019 I had the opportunity to talk to folks that attended the SF Kubernetes meetup <a href="https://www.meetup.com/San-Francisco-Kubernetes-Meetup/events/259713345/" rel="nofollow noreferrer" target="_blank">Anaplan</a> about kind!</p>

<p>It&rsquo;s a great project and I end up using kind everyday to validate or develop designs for Kubernetes clusters.</p>

<p>The slides that I presented are here: <a href="https://mauilion.github.com/kind-demo" rel="nofollow noreferrer" target="_blank">mauilion.github.io/kind-demo</a> and a link to the repository with the deck and the content used to bring up the demo cluster is here: <a href="https://github.com/mauilion/kind-demo" rel="nofollow noreferrer" target="_blank">github.com/mauilion/kind-demo</a></p>

<p><a href="https://mauilion.github.io/kind-demo" rel="nofollow noreferrer" target="_blank"><img src="/kind-slide.png" alt="" /></a></p>

<p>In the talk I dug in a bit about what kind and kubeadm are and what problems they solve.</p>

<p>I also demonstrated creating a 7 node cluster on my laptop live!</p>

<p>Finally, we spent a little time talking about the way that Docker in Docker is being used here.</p>

<p>My laptop is a recent Lenovo x1 carbon running Ubuntu and i3.</p>

<p>When I bring up a kind cluster I can see the docker containers that I start with a simple <code>docker ps</code></p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">$ docker ps --no-trunc 
CONTAINER ID                                                       IMAGE                  COMMAND                                  CREATED             STATUS              PORTS                                  NAMES
b8f8ef6d2d97836dc66e09fe5e1a4c7e1b7a880c95372b8d4881288238985f22   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes       <span class="m">36533</span>/tcp, <span class="m">127</span>.0.0.1:36533-&gt;6443/tcp   kind-external-load-balancer
69daaf381d8a4dbafb1197502446858e9b6e9e950c0b8db1eb1759dc2883f3ec   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes       <span class="m">34675</span>/tcp, <span class="m">127</span>.0.0.1:34675-&gt;6443/tcp   kind-control-plane3
9f577280b62052d5caeecd7483e3283f01d3a3c784c4620efca15338cd0cad23   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes       <span class="m">38847</span>/tcp, <span class="m">127</span>.0.0.1:38847-&gt;6443/tcp   kind-control-plane
dfcab2e279ffbb2710dbdaa3386814887d081ddd378641777116b3fed131a3b0   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                                              kind-worker
e486393a724079b77b4aaec5de18fd0aea70f9ce0b46bb6d45edb3382bf3cb32   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes       <span class="m">35759</span>/tcp, <span class="m">127</span>.0.0.1:35759-&gt;6443/tcp   kind-control-plane2
be76f1f1ba3c365a5058c2f46b555174c1c6b28418844621e31a2e2c548c5e5f   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                                              kind-worker2
5a845004c40b035a198333a7f8c17eec8c3a024db15f484af4b5d7974e4c27db   kindest/node:v1.13.4   <span class="s2">&#34;/usr/local/bin/entrypoint /sbin/init&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                                              kind-worker3</code></pre></div>
<p>And if I exec into one of the control plane &ldquo;nodes&rdquo; and run docker ps:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">root@kind-control-plane:/# docker ps
CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
0904a715c607        18ee25ef69a8           <span class="s2">&#34;kube-controller-man…&#34;</span>   <span class="m">11</span> minutes ago      Up <span class="m">11</span> minutes                           k8s_kube-controller-manager_kube-controller-manager-kind-control-plane_kube-system_0139f650b0ebdfe8039809598eafaed5_1
cce01b13d1be        fd722e321590           <span class="s2">&#34;kube-scheduler --ad…&#34;</span>   <span class="m">11</span> minutes ago      Up <span class="m">11</span> minutes                           k8s_kube-scheduler_kube-scheduler-kind-control-plane_kube-system_4b52d75cab61380f07c0c5a69fb371d4_1
adb83f623945        calico/node            <span class="s2">&#34;start_runit&#34;</span>            <span class="m">11</span> minutes ago      Up <span class="m">11</span> minutes                           k8s_calico-node_calico-node-bkbjv_kube-system_f3ffe8bb-5be3-11e9-a476-024240bbde2e_0
036e0f373c0b        7fe6f0b71640           <span class="s2">&#34;/usr/local/bin/kube…&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_kube-proxy_kube-proxy-vnmbc_kube-system_f4010699-5be3-11e9-a476-024240bbde2e_0
57b9c22fa25a        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_calico-node-bkbjv_kube-system_f3ffe8bb-5be3-11e9-a476-024240bbde2e_0
f8ccefbb6faf        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_kube-proxy-vnmbc_kube-system_f4010699-5be3-11e9-a476-024240bbde2e_0
3b722fb72dd3        4eb4a1578884           <span class="s2">&#34;kube-apiserver --au…&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_kube-apiserver_kube-apiserver-kind-control-plane_kube-system_36fd00068b02bdfc674c44e345a08553_0
37ce90751bb7        3cab8e1b9802           <span class="s2">&#34;etcd --advertise-cl…&#34;</span>   <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_etcd_etcd-kind-control-plane_kube-system_a17306e4c3c6a492df6a1ccea459c458_0
b2dab14dc554        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_kube-scheduler-kind-control-plane_kube-system_4b52d75cab61380f07c0c5a69fb371d4_0
aa56021201fb        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_kube-controller-manager-kind-control-plane_kube-system_0139f650b0ebdfe8039809598eafaed5_0
71d3e0cb6fe2        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_kube-apiserver-kind-control-plane_kube-system_36fd00068b02bdfc674c44e345a08553_0
8a2e80860798        k8s.gcr.io/pause:3.1   <span class="s2">&#34;/pause&#34;</span>                 <span class="m">12</span> minutes ago      Up <span class="m">12</span> minutes                           k8s_POD_etcd-kind-control-plane_kube-system_a17306e4c3c6a492df6a1ccea459c458_0</code></pre></div>
<p>and from the underlying node we can the processes that are related to the containers.</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"> <span class="m">2572</span> ?        Ssl    <span class="m">1</span>:44 /usr/bin/dockerd --live-restore -H fd://
 <span class="m">2655</span> ?        Ssl    <span class="m">1</span>:40  <span class="se">\_</span> docker-containerd --config /var/run/docker/containerd/containerd.toml
<span class="m">10669</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/9f577280b62052d5caeecd7483e3283f01d3a
<span class="m">10801</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>   <span class="se">\_</span> /sbin/init
<span class="m">14598</span> ?        S&lt;s    <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="se">\_</span> /lib/systemd/systemd-journald
<span class="m">14736</span> ?        Ssl    <span class="m">2</span>:18  <span class="p">|</span>   <span class="p">|</span>       <span class="se">\_</span> /usr/bin/dockerd -H fd://
<span class="m">14958</span> ?        Ssl    <span class="m">0</span>:33  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> docker-containerd --config /var/run/docker/containerd/containerd.toml
<span class="m">22752</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/8a2e8086079885ea914c5
<span class="m">22816</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">22762</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/71d3e0cb6fe2f988842bb
<span class="m">22852</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">22777</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/aa56021201fb02aa8d855
<span class="m">22846</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">22795</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/b2dab14dc554cdcf40e13
<span class="m">22881</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">23015</span> ?        Sl     <span class="m">0</span>:03  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/37ce90751bb7b196243f1
<span class="m">23061</span> ?        Ssl    <span class="m">4</span>:41  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> etcd --advertise-client-urls<span class="o">=</span>https://172.17.0.6:2379 --cert-file<span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt --client-cert-auth<span class="o">=</span><span class="nb">true</span> --data-dir
<span class="m">23066</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/3b722fb72dd30e8b3e07f
<span class="m">23126</span> ?        Ssl    <span class="m">5</span>:30  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> kube-apiserver --authorization-mode<span class="o">=</span>Node,RBAC --advertise-address<span class="o">=</span><span class="m">172</span>.17.0.6 --allow-privileged<span class="o">=</span><span class="nb">true</span> --client-ca-file<span class="o">=</span>/etc/kubernetes/p
<span class="m">24764</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/f8ccefbb6faf067876cf4
<span class="m">24830</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">24779</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/57b9c22fa25a83e4c69ca
<span class="m">24819</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /pause
<span class="m">24895</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/036e0f373c0bcac56484c
<span class="m">24921</span> ?        Ssl    <span class="m">0</span>:18  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /usr/local/bin/kube-proxy --config<span class="o">=</span>/var/lib/kube-proxy/config.conf --hostname-override<span class="o">=</span>kind-control-plane
<span class="m">26721</span> ?        Sl     <span class="m">0</span>:04  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/adb83f623945215c3597a
<span class="m">26746</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> /sbin/runsvdir -P /etc/service/enabled
<span class="m">28040</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> runsv bird6
<span class="m">28242</span> ?        S      <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg
<span class="m">28041</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> runsv confd
<span class="m">28047</span> ?        Sl     <span class="m">0</span>:28  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> calico-node -confd
<span class="m">28042</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> runsv felix
<span class="m">28044</span> ?        Sl     <span class="m">2</span>:03  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> calico-node -felix
<span class="m">28043</span> ?        Ss     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> runsv bird
<span class="m">28245</span> ?        S      <span class="m">0</span>:01  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>           <span class="se">\_</span> bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg
<span class="m">27663</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/cce01b13d1be8c0e434cb
<span class="m">27701</span> ?        Ssl    <span class="m">1</span>:19  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="p">|</span>   <span class="se">\_</span> kube-scheduler --address<span class="o">=</span><span class="m">127</span>.0.0.1 --kubeconfig<span class="o">=</span>/etc/kubernetes/scheduler.conf --leader-elect<span class="o">=</span><span class="nb">true</span>
<span class="m">27704</span> ?        Sl     <span class="m">0</span>:00  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>       <span class="se">\_</span> docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/0904a715c607d662900b1
<span class="m">27744</span> ?        Ssl    <span class="m">0</span>:04  <span class="p">|</span>   <span class="p">|</span>       <span class="p">|</span>           <span class="se">\_</span> kube-controller-manager --enable-hostpath-provisioner<span class="o">=</span><span class="nb">true</span> --address<span class="o">=</span><span class="m">127</span>.0.0.1 --allocate-node-cidrs<span class="o">=</span><span class="nb">true</span> --authentication-kubeconfig<span class="o">=</span>/</code></pre></div>
<p>This is because at each of the layers of abstraction, we are again still sharing the same linux kernel. So when I create containers leveraging something like docker in docker I am still making use of the same resources I would even if I were to run the docker command from the underlying node.</p>

<p>Put another way the docker daemon and all it&rsquo;s dependencies is running as an application inside the docker container I started. It&rsquo;s not mounting in the docker socket or any of that just making use of docker and the linux namespaces available to it.</p>

<p>Thanks!</p>
]]></content>
		</item>
		
		<item>
			<title>debugging tools: a preconfigured etcdclient static pod</title>
			<link>https://mauilion.dev/posts/etcdclient/</link>
			<pubDate>Mon, 18 Mar 2019 16:25:23 -0700</pubDate>
			
			<guid>https://mauilion.dev/posts/etcdclient/</guid>
			<description>&lt;p&gt;In this post I am going to discuss &lt;a href=&#34;https://git.io/etcdclient.yaml&#34; rel=&#34;nofollow noreferrer&#34; target=&#34;_blank&#34;&gt;git.io/etcdclient.yaml&lt;/a&gt; and why it&amp;rsquo;s neat!&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>In this post I am going to discuss <a href="https://git.io/etcdclient.yaml" rel="nofollow noreferrer" target="_blank">git.io/etcdclient.yaml</a> and why it&rsquo;s neat!</p>

<p>When putting together content for a series of blog posts that I am doing around etcd recovery and failure scenarios, I realized that I was configuring the etcdclient to interact with the etcd cluster that kubeadm stands up.</p>

<p>I wanted to create a static pod that would sit on the same node as the static pod that operates the etcd server so that I can use it to troubleshoot the etcd cluster that kubeadm is bringing up.</p>

<p><a href="https://git.io/etcdclient.yaml" rel="nofollow noreferrer" target="_blank">git.io/etcdclient.yaml</a> is an attempt to DRY (do not repeat yourself) work up.</p>

<p>It makes a set of assumptions.</p>

<ol>
<li>That etcd has been created by kubeadm as a <code>local etcd</code></li>
<li>That we have well defined locations for certs on the underlying file system layed down by kubeadm.</li>
<li>That etcd is listening on localhost and a node ip or for our purposes at the very least localhost.</li>
</ol>

<p>The static pod looks like:</p>

<script type="application/javascript" src="//gist.github.com/mauilion/2bab4b00eb7a0ab4fca7023ae251e8ee.js"></script>

<p>The interesting bits are the env vars that configure etcdclient on your behalf.</p>

<p>With etcd and etcdclient the arguments that you can pass at the cli are also <a href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/configuration.md" rel="nofollow noreferrer" target="_blank">exposed as environment variables</a>.</p>

<p>Now to see it in action!</p>

<p>
    <asciinema-player
        src="/casts/etcdclient.cast"
        cols="200"
        rows="30"
        
        preload="true"
        
        start-at="0"
        speed="1"
        
        poster="npt:1:00"
        
        
        
        
        
        
    ></asciinema-player>
</p>]]></content>
		</item>
		
	</channel>
</rss>
